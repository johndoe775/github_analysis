{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c0d9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "\"\"\"\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.graph import Graph, Node, StartNode, StopNode\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\"\"\"\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.environ[\"groq\"]\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_key\n",
    ")\n",
    "llm_ds= ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_key\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0468482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26187d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Taj Mahal is located in Agra, a city in the Indian state of Uttar Pradesh. It is situated on the southern bank of the Yamuna River, approximately 230 kilometers (143 miles) south of New Delhi, the capital city of India.\\n\\nThe exact address of the Taj Mahal is:\\n\\nTaj Mahal, Dharmapuri, Forest Colony, Tajganj, Agra, Uttar Pradesh 282001, India\\n\\nIt's one of the most famous and iconic landmarks in India, and a popular tourist destination, attracting millions of visitors each year.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"where is taj mahal?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05468958",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume,personal_info=\"\",\"\"\"Strategic and results-driven talent acquisition leader with a proven track record in recruiting and talent management across diverse industries. I specialize in designing and executing scalable recruitment strategies that align with organizational goals and elevate the candidate experience.\n",
    "\n",
    "With a deep understanding of data-driven hiring, I leverage analytics to streamline processes, enhance team performance, and drive informed decision-making. I’m not just focused on filling roles—I build and lead high-performing recruitment teams capable of managing high-volume, cross-functional hiring with precision and efficiency.\n",
    "\n",
    "My approach has consistently improved hiring outcomes, aligning recruitment operations with business objectives while ensuring an exceptional stakeholder experience. Backed by a strong foundation in Customer Relationship Management and Human Resources, I am passionate about delivering excellence in every aspect of talent acquisition.\"\"\"\n",
    "with open('/workspaces/github_analysis/data/job_description.txt', 'r') as file:\n",
    "    job_description=file.read()\n",
    "loader = PyPDFLoader(\"/workspaces/github_analysis/data/resume_with_capg.pdf\")\n",
    "documents = loader.load()\n",
    "for i in documents:\n",
    "    resume+=(i.page_content.strip())\n",
    "    \n",
    "with open('/workspaces/github_analysis/src/prompts.yaml', 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "    \n",
    "def intro():\n",
    "\n",
    "    print(\"*\"*100)\n",
    "    for i,j in enumerate(data.keys()):\n",
    "        print(f\"{i+1}. {j}\")\n",
    "def inputs():\n",
    "    i=input(\"Enter the number corresponding to the key you want to use: \")\n",
    "    print(\"selected number is \",int(i))\n",
    "    key=list(data.keys())[int(i)-1]\n",
    "    return key\n",
    "\n",
    "def func(key):\n",
    "    input_variables=list(data[key][\"inputs\"].keys())\n",
    "    template=data[key][\"prompt\"].format(resume=resume, job_description=job_description,personal_info=personal_info)\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=input_variables,\n",
    "        template=template\n",
    "    )\n",
    "    print(prompt_template,input_variables)\n",
    "    chain=prompt_template | llm\n",
    "    response=chain.invoke(data[key][\"inputs\"]).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93da96c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "1. resume_and_job_decription_rewrite\n",
      "2. cold_mail_and_person_info\n",
      "3. cold_mail_no_personal_info\n"
     ]
    }
   ],
   "source": [
    "intro()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0809b6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected number is  2\n"
     ]
    }
   ],
   "source": [
    "x=inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58369e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} template='\"\"\"\\nUsing the following resume, write a professional and inquisitive cold mail tailored to the provided person\\'s information, seeking job opportunities:\\n\\nResume: Dinesh\\tSagar\\n+91-8121909150\\n\\t\\nMail\\n\\t\\nGitHub\\n\\t\\nLinkedIn\\tProfile\\nPassionate\\tData\\tScientist\\twith\\ta\\tdrive\\tto\\ttackle\\treal-world\\tchallenges\\tthrough\\tdata\\tanalysis\\tand\\tmachine\\tlearning.\\nCommitted\\tto\\textracting\\tvalue\\tfrom\\tcomplex\\tdata\\tand\\tdelivering\\tactionable\\tinsights\\tto\\tinform\\tbusiness\\tdecisions\\t\\nand\\ndrive\\tgrowth.\\tEager\\tto\\tcontinuously\\tlearn\\tand\\timprove\\tto\\tdeliver\\timpactful\\tsolutions.\\nExperience\\nCapgemini\\nConsultant\\nFEB\\t2025-\\tPRESENT\\nAmazon\\tDevelopment\\tCentre\\tIndia\\nQuality\\tSpecialist\\nJAN\\t2020\\t-\\tDEC\\t2024\\nImplemented\\tan\\toutlier\\tdetection\\tprogram\\tto\\nenhance\\taddress\\tdata\\tquality,\\timproving\\tlogistics\\nperformance\\tfor\\tAmazon\\tLogistics.\\nAnalyzed\\tlarge-scale\\tdatasets\\tusing\\tstatistical\\nmethods\\tto\\toptimize\\tdelivery\\toperations\\tand\\nresource\\tplanning.\\nBuilt\\ttime\\tseries\\tmodels\\tto\\tforecast\\tpeak\\torder\\nvolumes,\\tenabling\\tproactive\\tlogistics\\nmanagement.\\nDeveloped\\tmachine\\tlearning\\tmodels\\tto\\tevaluate\\ndelivery\\tagent\\tperformance,\\tdriving\\toperational\\nefficiency.\\nCreated\\tinteractive\\tdashboards\\tfor\\treal-time\\nmonitoring\\tand\\tactionable\\tinsights.\\nSkills\\nProgramming:\\n\\tPython,\\tSQL\\nData\\tVisualization:\\n\\tTableau,\\tPower\\tBI\\nStatistics\\t&\\tMath:\\n\\tStatistics\\tand\\tHypothesis\\nTesting,\\tNumPy,\\tPandas,\\tScikit-Learn\\nMachine\\tLearning:\\n\\tTensorFlow,\\tKeras,\\nStreamlit,\\tFlask,\\tFastAPI\\nGenerative\\tAI:\\n\\tLangChain,\\tLangGraph,\\tAWS\\nBedrock,\\tAzure\\tAI\\tFoundry,\\tAzure\\tMachine\\nLearning,\\tAWS\\tSagemaker\\nScaler\\tCertifications\\nPython\\tLibraries,\\tEDA\\tFundamentals\\nSupervised\\t&\\tUnsupervised\\tLearning\\nNatural\\tLanguage\\tProcessing,\\tComputer\\tVision\\nProjects\\nGenerative\\tAI\\tPowered\\tData\\tAssistants\\nMedical\\tRAG\\tagent:\\n\\tDeveloped\\ta\\tmedical\\tchatbot\\tusing\\nPinecone\\tDB\\tfor\\tvector\\tstorage\\tand\\tFlask\\tfor\\treal-time\\tquery\\nhandling\\tto\\tprovide\\taccurate,\\tcontext-driven\\tmedical\\nanswers.\\nRAG\\tAgent\\twith\\tChromaDB:\\n\\tDeveloped\\tan\\tintelligent\\nrecommendation\\tsystem\\tleveraging\\tChromaDB\\tfor\\treranking\\nand\\tcontent\\tdiscovery\\tbased\\ton\\tuser\\tqueries.\\nCSV\\tCommand\\tAgent:\\n\\tBuilt\\tan\\tAI-powered\\ttool\\tenabling\\nnatural\\tlanguage\\tmanipulation\\tof\\tCSV\\tfiles\\tand\\tSQL\\tquery\\ngeneration,\\tsimplifying\\tdata\\toperations\\tfor\\tnon-technical\\nusers.\\nNamed\\tEntity\\tRecognition\\t(NER)\\tin\\tTweets\\nUsing\\tLSTM+CRF\\tand\\tBERT\\tModels\\nDeveloped\\ta\\trobust\\tNamed\\tEntity\\tRecognition\\t(NER)\\tsystem\\nfor\\ttweets\\tusing\\tLSTM-CRF\\tand\\tBERT\\tmodels.\\nInitialized\\tLSTM-CRF\\twith\\tWord2Vec\\tembeddings\\tand\\tfine-\\ntuned\\tBERT\\tfor\\tentity\\tdetection.\\nExecuted\\tdata\\tpreprocessing,\\thyperparameter\\toptimization,\\nand\\tmodel\\tevaluation\\tto\\tboost\\taccuracy.\\nAchieved\\tprecise\\tentity\\trecognition\\ton\\treal-world\\ttweet\\tdata,\\nensuring\\thigh\\tmodel\\tperformance.\\nNinjacart\\tComputer\\tVision\\tProject\\nDeveloped\\ta\\tCNN-based\\tcomputer\\tvision\\tsystem\\tfor\\nNinjacart\\tto\\tidentify\\tonions,\\tpotatoes,\\ttomatoes,\\tand\\tnoise\\tin\\nproduce\\timages.\\nLeveraged\\ttransfer\\tlearning\\t(VGG,\\tResNet)\\tand\\tdata\\naugmentation,\\twith\\toptimized\\ttraining\\tvia\\tcallbacks\\tand\\nregularization\\ttechniques.\\nAchieved\\trobust\\tand\\taccurate\\tvegetable\\tclassification.\\nDelivery\\tTime\\tEstimation\\tfor\\tPorter\\'s\\tDelivery\\nService\\nPerformed\\tcomprehensive\\tdata\\tcleaning,\\tfeature\\nengineering,\\tand\\tcategorical\\tencoding\\nDesigned,\\ttrained,\\tand\\toptimized\\ta\\tneural\\tnetwork\\tmodel\\nusing\\tTensorFlow\\tand\\tKeras\\nDelivered\\taccurate\\tdelivery\\ttime\\tpredictions,\\tenhancing\\nservice\\tquality\\tand\\toperational\\tefficiency\\nAdEase\\tWiki\\tPage\\tView\\tForecasting\\nForecasted\\tWikipedia\\tpage\\tviews\\tto\\toptimize\\tad\\tplacement\\nfor\\tAdEase\\tclients,\\tconsidering\\tregional\\tand\\tlinguistic\\nvariations.\\nApplied\\tARIMA,\\tSARIMAX,\\tand\\tProphet\\ttime\\tseries\\tmodels,\\nincorporating\\texogenous\\tcampaign\\tdata\\tfor\\tenhanced\\naccuracy.\\nImproved\\tad\\tplacement\\tstrategies\\tbased\\ton\\tpredicted\\tpage\\nview\\ttrends,\\tenabling\\tbetter\\tROI\\tand\\ttargeted\\tadvertising\\ncampaigns\\tfor\\tdiverse\\tregions.Education\\nScaler\\nData\\tScience\\tand\\tMachine\\tLearning\\n2024\\nChaitanya\\tBharathi\\tInstitute\\tof\\nTechnology\\nB.E(Mechanical)\\n2019\\nZEE\\tRecommendation\\tSystem\\nImplemented\\ta\\tmovie\\trecommender\\tsystem\\tfor\\tpersonalized\\nmovie\\tsuggestions.\\nUtilized\\tuser-user\\tand\\titem-item\\tsimilarity\\twith\\tCosine\\tand\\nPearson\\tmethods.\\nBuilt\\tcollaborative\\tfiltering\\tand\\tmatrix\\tfactorization\\tmodels\\nfor\\trecommendations.\\nEnhanced\\tuser\\texperience\\tthrough\\tdata-driven\\tmovie\\nsuggestions\\nGraduate\\tAdmissions\\tPrediction\\nAnalyzed\\tgraduate\\tadmissions\\tdata,\\tconstructed\\tprecise\\nlinear\\tregression\\tmodels\\tin\\tPython,\\tand\\tpresented\\tactionable\\ninsights\\tto\\tenhance\\tstudent\\tsuccess\\tfor\\tJamboree.\\nCreated\\tLinear\\tregression\\tmodel\\tand\\timplemented\\tRidge\\tand\\nLasso\\tregularizations\\tand\\tevaluated\\tmodel\\tmetrics\\tusing\\nsklearn\\tstats\\tlibrary.\\tIdentified\\tsignificant\\tpredictors\\t(e.g.,\\nGRE\\tscores,\\tGPA)\\tfor\\tgraduate\\tadmissions.\\nDeveloped\\taccurate\\tlinear\\tregression\\tmodels\\twith\\tR2\\tscore\\nexceeding\\t0.8.\\nDelivered\\tinsights\\tand\\trecommendations\\tfor\\timproving\\napplication\\tprofiles.\\nLoanTap\\tCreditworthiness\\tModel\\nDeveloped\\ta\\tLogistic\\tRegression\\tmodel\\tfor\\tLoanTap,\\nanalyzed\\tprecision\\tvs.\\trecall\\ttradeoffs,\\tand\\tprovided\\nactionable\\tinsights\\tfor\\tenhanced\\tloan\\tdecisions.\\nCreated\\tseveral\\tflags\\tand\\tindicators\\tbased\\ton\\tthe\\tgiven\\nfeatures\\tand\\tapplied\\ta\\tlogistic\\tregression\\tmodel\\tto\\tgenerate\\nclassification\\treport,\\tROC\\tAUC\\tcurve,\\tprecision-recall\\tcurve.\\nRevealed\\tkey\\trelationships\\tbetween\\tloan\\tstatus\\tand\\nborrower\\tattributes.\\nAchieved\\thigh\\tAUC\\tscore,\\tindicating\\tstrong\\tmodel\\nperformance.\\nProvided\\tinsights\\tinto\\tsignificant\\tpredictors\\tof\\tloan\\tdefaults,\\nleading\\tto\\timproved\\tcreditworthiness\\tassessment.\\n\\nPerson Information: Strategic and results-driven talent acquisition leader with a proven track record in recruiting and talent management across diverse industries. I specialize in designing and executing scalable recruitment strategies that align with organizational goals and elevate the candidate experience.\\n\\nWith a deep understanding of data-driven hiring, I leverage analytics to streamline processes, enhance team performance, and drive informed decision-making. I’m not just focused on filling roles—I build and lead high-performing recruitment teams capable of managing high-volume, cross-functional hiring with precision and efficiency.\\n\\nMy approach has consistently improved hiring outcomes, aligning recruitment operations with business objectives while ensuring an exceptional stakeholder experience. Backed by a strong foundation in Customer Relationship Management and Human Resources, I am passionate about delivering excellence in every aspect of talent acquisition.\\n\\nInstructions:\\n0) write in first person.\\n1) Use the available experience, projects, and skills from the resume to write the cold mail.\\n2) The tone must be professional and inquisitive, seeking job opportunities.\\n3) Don\\'t hallucinate.\\n4) Don\\'t use any external data but only the resume and person information.\\n5) Don\\'t mention any comments and preamble in the response.\\n6) Make sure that the entire message is conveyed in not more than 5 lines.\\n7) End with regards.\\n\"\"\"\\n' ['resume', 'personal_info']\n",
      "I'm a passionate Data Scientist with experience in machine learning and data analysis, having worked on projects like Generative AI Powered Data Assistants and Medical RAG agent. I'm eager to leverage my skills in Python, TensorFlow, and Keras to drive business growth. I'm interested in exploring opportunities with your organization. Can we discuss potential openings? \n",
      "Regards\n"
     ]
    }
   ],
   "source": [
    "print(func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b850fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71163108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/github_analysis/research'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "202b075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/workspaces/github_analysis/\")\n",
    "from src.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68eaf4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resume_and_job_decription_rewrite': {'tool_use_case': 'use this tool to rewrite a resume based on a job description provided.',\n",
       "  'prompt': '\"\"\"\\nRewrite the following resume to match the job description provided:\\n\\nResume: {resume}\\n\\nJob Description: {job_description}\\n\\nInstructions:\\n1) Use the available experience, projects, and skills to rewrite the resume.\\n2) Don\\'t Halucinate.\\n3) Don\\'t use any external data but only the resume and job description.\\n4) Don\\'t mention any comments and preamble in the response.\\n\"\"\"\\n',\n",
       "  'inputs': {'resume': 'resume', 'job_description': 'job_description'}},\n",
       " 'cold_mail_and_person_info': {'tool_use_case': \"use this tool to write a professional and inquisitive cold mail tailored to a person's information, seeking job opportunities.\",\n",
       "  'prompt': '\"\"\"\\nUsing the following resume, write a professional and inquisitive cold mail tailored to the provided person\\'s information, seeking job opportunities:\\n\\nResume: {resume}\\n\\nPerson Information: {personal_info}\\n\\nInstructions:\\n0) write in first person.\\n1) Use the available experience, projects, and skills from the resume to write the cold mail.\\n2) The tone must be professional and inquisitive, seeking job opportunities.\\n3) Don\\'t hallucinate.\\n4) Don\\'t use any external data but only the resume and person information.\\n5) Don\\'t mention any comments and preamble in the response.\\n6) Make sure that the entire message is conveyed in not more than 5 lines.\\n7) End with regards.\\n\"\"\"\\n',\n",
       "  'inputs': {'resume': 'resume', 'personal_info': 'personal_info'}},\n",
       " 'cold_mail_no_personal_info': {'tool_use_case': 'use this tool to write a generic professional and inquisitive cold mail seeking job opportunities without any personal information about the recipient.',\n",
       "  'prompt': '\"\"\"\\nUsing the following resume, write a generic professional and inquisitive cold mail seeking job opportunities. No personal information about the recipient is provided.\\n\\nResume: {resume}\\n\\nInstructions:\\n0) write in first person.\\n1) Use the available experience, projects, and skills from the resume to write the cold mail.\\n2) The tone must be professional and inquisitive, seeking job opportunities.\\n3) Don\\'t hallucinate.\\n4) Don\\'t use any external data but only the resume.\\n5) Don\\'t mention any comments and preamble in the response.\\n6) Make sure that the entire message is conveyed in not more than 5 lines.\\n7) End with regards.\\n\"\"\"\\n',\n",
       "  'inputs': {'resume': 'resume'}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "creating tools and binding them to the LLM\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc28657b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['resume_and_job_decription_rewrite', 'cold_mail_and_person_info', 'cold_mail_no_personal_info'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67a5b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "def make_tool_func(key):\n",
    "    code = f'''\n",
    "@tool\n",
    "def {key}():\n",
    "    \"\"\"\n",
    "    Description\n",
    "    {data[key][\"tool_use_case\"]}\n",
    "    \"\"\"\n",
    "    input_variables = list(data[key][\"inputs\"].keys())\n",
    "    template = data[key][\"prompt\"].format(\n",
    "        resume=resume, job_description=job_description, personal_info=personal_info\n",
    "    )\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=input_variables,\n",
    "        template=template\n",
    "    )\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke(data[key][\"inputs\"]).content\n",
    "    return response\n",
    "'''\n",
    "    local_vars = {}\n",
    "    exec(code, globals(), local_vars)\n",
    "    return local_vars[key]\n",
    "\n",
    "for key in data.keys():\n",
    "    globals()[key] = make_tool_func(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6087c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80d9da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1=llm_ds.bind_tools(tools=[add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe53a3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is 2+2?\" So, I need to figure out how to respond using the tools provided. \\n\\nLooking at the tools, there\\'s an \"add\" function that takes two integers, a and b, and returns their sum. That seems perfect for this question.\\n\\nI should call the add function with a=2 and b=2. That should give me the correct result of 4.\\n\\nI need to format the response as a JSON object within the tool_call XML tags. So, the function name is \"add\" and the arguments should be an object with a and b both set to 2.\\n\\nDouble-checking the function parameters to make sure I\\'m using the right keys and data types. Everything looks good.\\n\\nAlright, time to put it all together in the required format.\\n', 'tool_calls': [{'id': 'vmcqcgc5w', 'function': {'arguments': '{\"a\":2,\"b\":2}', 'name': 'add'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 201, 'prompt_tokens': 134, 'total_tokens': 335, 'completion_time': 0.817864261, 'prompt_time': 0.008567887, 'queue_time': 0.05860929300000001, 'total_time': 0.826432148}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--42e0df24-d30d-4d32-b119-b4f62c2c6e8b-0' tool_calls=[{'name': 'add', 'args': {'a': 2, 'b': 2}, 'id': 'vmcqcgc5w', 'type': 'tool_call'}] usage_metadata={'input_tokens': 134, 'output_tokens': 201, 'total_tokens': 335}\n"
     ]
    }
   ],
   "source": [
    "response = llm1.invoke(\"What is 2+2?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2746d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1=llm_ds.bind_tools(tools=[globals()[key] for key in data.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "086c6624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': \"Alright, let me try to figure out how to approach this user's query. They've provided a resume for Dinesh Sagar and a personal info section for someone in talent acquisition. They want to write a cold mail, so I need to determine which tool to use.\\n\\nLooking at the tools available, there's one for when personal info is available and another for when it's not. Since the user provided personal info about the recipient, I should use the cold_mail_and_person_info tool.\\n\\nNow, I need to structure the arguments correctly. The resume and personal info should be passed as separate strings. I'll make sure to format them properly in JSON, escaping any special characters if necessary.\\n\\nI should also ensure that the function call is correctly wrapped in the tool_call XML tags as specified. Double-checking the function name and the structure of the arguments is crucial to avoid any errors.\\n\\nFinally, I'll present the tool_call with the appropriate JSON arguments, making sure it's clear and correctly formatted for the system to process.\\n\", 'tool_calls': [{'id': 'qexag2ycq', 'function': {'arguments': '{\"personal_info\":\"Strategic and results-driven talent acquisition leader with a proven track record in recruiting and talent management across diverse industries. I specialize in designing and executing scalable recruitment strategies that align with organizational goals and elevate the candidate experience.\\\\n\\\\nWith a deep understanding of data-driven hiring, I leverage analytics to streamline processes, enhance team performance, and drive informed decision-making. I\\'m not just focused on filling roles\\'\\'I build and lead high-performing recruitment teams capable of managing high-volume, cross-functional hiring with precision and efficiency.\\\\n\\\\nMy approach has consistently improved hiring outcomes, aligning recruitment operations with business objectives while ensuring an exceptional stakeholder experience. Backed by a strong foundation in Customer Relationship Management and Human Resources, I am passionate about delivering excellence in every aspect of talent acquisition.\",\"resume\":\"Dinesh Sagar\\\\n+91-8121909150\\\\n\\\\nMail\\\\n\\\\nGitHub\\\\n\\\\nLinkedIn Profile\\\\nPassionate Data Scientist with a drive to tackle real-world challenges through data analysis and machine learning.\\\\nCommitted to extracting value from complex data and delivering actionable insights to inform business decisions and drive growth. Eager to continuously learn and improve to deliver impactful solutions.\\\\n\\\\nExperience\\\\nCapgemini\\\\nConsultant\\\\nFEB 2025- PRESENT\\\\nAmazon Development Centre India\\\\nQuality Specialist\\\\nJAN 2020 - DEC 2024\\\\nImplemented an outlier detection program to enhance address data quality, improving logistics performance for Amazon Logistics.\\\\nAnalyzed large-scale datasets using statistical methods to optimize delivery operations and resource planning.\\\\nBuilt time series models to forecast peak order volumes, enabling proactive logistics management.\\\\nDeveloped machine learning models to evaluate delivery agent performance, driving operational efficiency.\\\\nCreated interactive dashboards for real-time monitoring and actionable insights.\\\\n\\\\nSkills\\\\nProgramming: Python, SQL\\\\nData Visualization: Tableau, Power BI\\\\nStatistics \\\\u0026 Math: Statistics and Hypothesis Testing, NumPy, Pandas, Scikit-Learn\\\\nMachine Learning: TensorFlow, Keras, Streamlit, Flask, FastAPI\\\\nGenerative AI: LangChain, LangGraph, AWS Bedrock, Azure AI Foundry, Azure Machine Learning, AWS Sagemaker\\\\n\\\\nProjects\\\\nGenerative AI Powered Data Assistants\\\\nMedical RAG agent: Developed a medical chatbot using Pinecone DB for vector storage and Flask for real-time query handling to provide accurate, context-driven medical answers.\\\\nRAG Agent with ChromaDB: Developed an intelligent recommendation system leveraging ChromaDB for reranking and content discovery based on user queries.\\\\nCSV Command Agent: Built an AI-powered tool enabling natural language manipulation of CSV files and SQL query generation, simplifying data operations for non-technical users.\\\\nNamed Entity Recognition (NER) in Tweets Using LSTM+CRF and BERT Models\\\\nDeveloped a robust Named Entity Recognition (NER) system for tweets using LSTM-CRF and BERT models.\\\\nInitialized LSTM-CRF with Word2Vec embeddings and fine-tuned BERT for entity detection.\\\\nExecuted data preprocessing, hyperparameter optimization, and model evaluation to boost accuracy.\\\\nAchieved precise entity recognition on real-world tweet data, ensuring high model performance.\\\\n\\\\nNinjacart Computer Vision Project\\\\nDeveloped a CNN-based computer vision system for Ninjacart to identify onions, potatoes, tomatoes, and noise in produce images.\\\\nLeveraged transfer learning (VGG, ResNet) and data augmentation, with optimized training via callbacks and regularization techniques.\\\\nAchieved robust and accurate vegetable classification.\\\\n\\\\nDelivery Time Estimation for Porter\\'s Delivery Service\\\\nPerformed comprehensive data cleaning, feature engineering, and categorical encoding\\\\nDesigned, trained, and optimized a neural network model using TensorFlow and Keras\\\\nDelivered accurate delivery time predictions, enhancing service quality and operational efficiency\\\\n\\\\nAdEase Wiki Page View Forecasting\\\\nForecasted Wikipedia page views to optimize ad placement for AdEase clients, considering regional and linguistic variations.\\\\nApplied ARIMA, SARIMAX, and Prophet time series models, incorporating exogenous campaign data for enhanced accuracy.\\\\nImproved ad placement strategies based on predicted page view trends, enabling better ROI and targeted advertising campaigns for diverse regions.\\\\n\\\\nEducation\\\\nScaler Data Science and Machine Learning 2024\\\\nChaitanya Bharathi Institute of Technology B.E(Mechanical) 2019\\\\n\\\\nProjects\\\\nZEE Recommendation System\\\\nImplemented a movie recommender system for personalized movie suggestions.\\\\nUtilized user-user and item-item similarity with Cosine and Pearson methods.\\\\nBuilt collaborative filtering and matrix factorization models for recommendations.\\\\nEnhanced user experience through data-driven movie suggestions\\\\n\\\\nGraduate Admissions Prediction\\\\nAnalyzed graduate admissions data, constructed precise linear regression models in Python, and presented actionable insights to enhance student success for Jamboree.\\\\nCreated Linear regression model and implemented Ridge and Lasso regularizations and evaluated model metrics using sklearn stats library. Identified significant predictors (e.g., GRE scores, GPA) for graduate admissions.\\\\nDeveloped accurate linear regression models with R2 score exceeding 0.8.\\\\nDelivered insights and recommendations for improving application profiles.\\\\n\\\\nLoanTap Creditworthiness Model\\\\nDeveloped a Logistic Regression model for LoanTap, analyzed precision vs. recall tradeoffs, and provided actionable insights for enhanced loan decisions.\\\\nCreated several flags and indicators based on the given features and applied a logistic regression model to generate classification report, ROC AUC curve, precision-recall curve.\\\\nRevealed key relationships between loan status and borrower attributes.\\\\nAchieved high AUC score, indicating strong model performance.\\\\nProvided insights into significant predictors of loan defaults, leading to improved creditworthiness assessment.\"}', 'name': 'cold_mail_and_person_info'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 1388, 'prompt_tokens': 1831, 'total_tokens': 3219, 'completion_time': 5.047272727, 'prompt_time': 0.133258931, 'queue_time': 0.059757749, 'total_time': 5.180531658}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e09b71b1-9a13-440e-876a-8e497106754d-0', tool_calls=[{'name': 'cold_mail_and_person_info', 'args': {'personal_info': \"Strategic and results-driven talent acquisition leader with a proven track record in recruiting and talent management across diverse industries. I specialize in designing and executing scalable recruitment strategies that align with organizational goals and elevate the candidate experience.\\n\\nWith a deep understanding of data-driven hiring, I leverage analytics to streamline processes, enhance team performance, and drive informed decision-making. I'm not just focused on filling roles''I build and lead high-performing recruitment teams capable of managing high-volume, cross-functional hiring with precision and efficiency.\\n\\nMy approach has consistently improved hiring outcomes, aligning recruitment operations with business objectives while ensuring an exceptional stakeholder experience. Backed by a strong foundation in Customer Relationship Management and Human Resources, I am passionate about delivering excellence in every aspect of talent acquisition.\", 'resume': \"Dinesh Sagar\\n+91-8121909150\\n\\nMail\\n\\nGitHub\\n\\nLinkedIn Profile\\nPassionate Data Scientist with a drive to tackle real-world challenges through data analysis and machine learning.\\nCommitted to extracting value from complex data and delivering actionable insights to inform business decisions and drive growth. Eager to continuously learn and improve to deliver impactful solutions.\\n\\nExperience\\nCapgemini\\nConsultant\\nFEB 2025- PRESENT\\nAmazon Development Centre India\\nQuality Specialist\\nJAN 2020 - DEC 2024\\nImplemented an outlier detection program to enhance address data quality, improving logistics performance for Amazon Logistics.\\nAnalyzed large-scale datasets using statistical methods to optimize delivery operations and resource planning.\\nBuilt time series models to forecast peak order volumes, enabling proactive logistics management.\\nDeveloped machine learning models to evaluate delivery agent performance, driving operational efficiency.\\nCreated interactive dashboards for real-time monitoring and actionable insights.\\n\\nSkills\\nProgramming: Python, SQL\\nData Visualization: Tableau, Power BI\\nStatistics & Math: Statistics and Hypothesis Testing, NumPy, Pandas, Scikit-Learn\\nMachine Learning: TensorFlow, Keras, Streamlit, Flask, FastAPI\\nGenerative AI: LangChain, LangGraph, AWS Bedrock, Azure AI Foundry, Azure Machine Learning, AWS Sagemaker\\n\\nProjects\\nGenerative AI Powered Data Assistants\\nMedical RAG agent: Developed a medical chatbot using Pinecone DB for vector storage and Flask for real-time query handling to provide accurate, context-driven medical answers.\\nRAG Agent with ChromaDB: Developed an intelligent recommendation system leveraging ChromaDB for reranking and content discovery based on user queries.\\nCSV Command Agent: Built an AI-powered tool enabling natural language manipulation of CSV files and SQL query generation, simplifying data operations for non-technical users.\\nNamed Entity Recognition (NER) in Tweets Using LSTM+CRF and BERT Models\\nDeveloped a robust Named Entity Recognition (NER) system for tweets using LSTM-CRF and BERT models.\\nInitialized LSTM-CRF with Word2Vec embeddings and fine-tuned BERT for entity detection.\\nExecuted data preprocessing, hyperparameter optimization, and model evaluation to boost accuracy.\\nAchieved precise entity recognition on real-world tweet data, ensuring high model performance.\\n\\nNinjacart Computer Vision Project\\nDeveloped a CNN-based computer vision system for Ninjacart to identify onions, potatoes, tomatoes, and noise in produce images.\\nLeveraged transfer learning (VGG, ResNet) and data augmentation, with optimized training via callbacks and regularization techniques.\\nAchieved robust and accurate vegetable classification.\\n\\nDelivery Time Estimation for Porter's Delivery Service\\nPerformed comprehensive data cleaning, feature engineering, and categorical encoding\\nDesigned, trained, and optimized a neural network model using TensorFlow and Keras\\nDelivered accurate delivery time predictions, enhancing service quality and operational efficiency\\n\\nAdEase Wiki Page View Forecasting\\nForecasted Wikipedia page views to optimize ad placement for AdEase clients, considering regional and linguistic variations.\\nApplied ARIMA, SARIMAX, and Prophet time series models, incorporating exogenous campaign data for enhanced accuracy.\\nImproved ad placement strategies based on predicted page view trends, enabling better ROI and targeted advertising campaigns for diverse regions.\\n\\nEducation\\nScaler Data Science and Machine Learning 2024\\nChaitanya Bharathi Institute of Technology B.E(Mechanical) 2019\\n\\nProjects\\nZEE Recommendation System\\nImplemented a movie recommender system for personalized movie suggestions.\\nUtilized user-user and item-item similarity with Cosine and Pearson methods.\\nBuilt collaborative filtering and matrix factorization models for recommendations.\\nEnhanced user experience through data-driven movie suggestions\\n\\nGraduate Admissions Prediction\\nAnalyzed graduate admissions data, constructed precise linear regression models in Python, and presented actionable insights to enhance student success for Jamboree.\\nCreated Linear regression model and implemented Ridge and Lasso regularizations and evaluated model metrics using sklearn stats library. Identified significant predictors (e.g., GRE scores, GPA) for graduate admissions.\\nDeveloped accurate linear regression models with R2 score exceeding 0.8.\\nDelivered insights and recommendations for improving application profiles.\\n\\nLoanTap Creditworthiness Model\\nDeveloped a Logistic Regression model for LoanTap, analyzed precision vs. recall tradeoffs, and provided actionable insights for enhanced loan decisions.\\nCreated several flags and indicators based on the given features and applied a logistic regression model to generate classification report, ROC AUC curve, precision-recall curve.\\nRevealed key relationships between loan status and borrower attributes.\\nAchieved high AUC score, indicating strong model performance.\\nProvided insights into significant predictors of loan defaults, leading to improved creditworthiness assessment.\"}, 'id': 'qexag2ycq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1831, 'output_tokens': 1388, 'total_tokens': 3219})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm1.invoke(f\"\"\"write a cold mail when resume {resume} and personal _info {personal_info} is given\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf92ac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1825016327.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "now  write a langgraph code that uses the tools defined above to create a workflow for generating a cold email based on the resume and personal info provided.\n",
    "it has one start onde ,input nodes that takes input from the user , three tool modes , and then the  end nodes ,using edged and conditinal edges between llm and tool nodes\n",
    "the conditional edges are defined by a function that redirects to the tools required based on the output given the input node \"\"\"\n",
    "def create_workflow():\n",
    "    from langchain_core.graph import Graph, Node, StartNode, StopNode\n",
    "    from langchain_core.graph import Edge, ConditionalEdge\n",
    "\n",
    "    graph = Graph()\n",
    "\n",
    "    start_node = StartNode(name=\"Start\")\n",
    "    input_node = Node(name=\"Input\", input_variables=[\"resume\", \"personal_info\"])\n",
    "    tool_nodes = [Node(name=key) for key in data.keys()]\n",
    "    end_node = StopNode(name=\"End\")\n",
    "\n",
    "    graph.add_nodes([start_node, input_node, *tool_nodes, end_node])\n",
    "\n",
    "    graph.add_edges([\n",
    "        Edge(start_node, input_node),\n",
    "        *[ConditionalEdge(input_node, tool_node, condition=lambda x: x == tool_node.name) for tool_node in tool_nodes],\n",
    "        *[Edge(tool_node, end_node) for tool_node in tool_nodes]\n",
    "    ])\n",
    "\n",
    "    return graph    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170948a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
